{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#library to create a dictionaty\n",
    "import gensim\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import training data\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/270168/Downloads/personal/nlp-starter-test/social_media_clean_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our deeds are the reason of this  earthquake m...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heard about  earthquake is different cities, s...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forest fire near la ronge sask  canada</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text choose_one  class_label\n",
       "0                 just happened a terrible car crash   Relevant            1\n",
       "1  our deeds are the reason of this  earthquake m...   Relevant            1\n",
       "2  heard about  earthquake is different cities, s...   Relevant            1\n",
       "3  there is a forest fire at spot pond, geese are...   Relevant            1\n",
       "4             forest fire near la ronge sask  canada   Relevant            1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the data \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate text column for preprocessing\n",
    "\n",
    "df[\"processed_text\"] = df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process (G):\n",
    "    tok = G.split(\" \")\n",
    "    stop_words = stopwords.words('english')\n",
    "    no_tok = [t for t in tok if t not in stop_words]\n",
    "    word_lemmat = WordNetLemmatizer()\n",
    "    lemmat_words = [word_lemmat.lemmatize(t) for t in no_tok]\n",
    "    return lemmat_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"processed_text\"] = df[\"processed_text\"].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        [happened, terrible, car, crash]\n",
       "1       [deed, reason, , earthquake, may, allah, forgi...\n",
       "2       [heard, , earthquake, different, cities,, stay...\n",
       "3       [forest, fire, spot, pond,, goose, fleeing, ac...\n",
       "4         [forest, fire, near, la, ronge, sask, , canada]\n",
       "5       [resident, asked, 'shelter, place', notified, ...\n",
       "6       [13,000, people, receive, , wildfire, evacuati...\n",
       "7       [got, sent, photo, ruby, , alaska, smoke, , wi...\n",
       "8       [, rockyfire, update, , , , california, hwy, ,...\n",
       "9         [apocalypse, lighting, , , spokane, , wildfire]\n",
       "10      [, flood, , disaster, heavy, rain, cause, flas...\n",
       "11           [typhoon, soudelor, kill, 28, china, taiwan]\n",
       "12                       [we're, shaking, , , earthquake]\n",
       "13                [i'm, top, hill, see, fire, wood, , , ]\n",
       "14      [there's, emergency, evacuation, happening, bu...\n",
       "15             [i'm, afraid, tornado, coming, area, , , ]\n",
       "16                 [three, people, died, heat, wave, far]\n",
       "17      [haha, south, tampa, getting, flooded, hah, , ...\n",
       "18              [, flood, bago, myanmar, , arrived, bago]\n",
       "19      [damage, school, bus, 80, multi, car, crash, ,...\n",
       "20      [they'd, probably, still, show, life, arsenal,...\n",
       "21                                           [hey!, you?]\n",
       "22                                         [what's, man?]\n",
       "23                                          [love, fruit]\n",
       "24                                       [summer, lovely]\n",
       "25                                            [car, fast]\n",
       "26                                           [nice, hat?]\n",
       "27                                [goooooooaaaaaal!!!!!!]\n",
       "28                                           [fuck, off!]\n",
       "29                                          [like, cold!]\n",
       "                              ...                        \n",
       "9252                                 [siren, everywhere!]\n",
       "9253    [breaking, , , isi, claim, responsibility, mos...\n",
       "9254                                    [omg, earthquake]\n",
       "9255    [official, , alabama, home, quarantined, possi...\n",
       "9256    [group, suicide, bomber, detonated, explosive,...\n",
       "9257    [heard, really, loud, bang, everyone, asleep, ...\n",
       "9258    [gas, thing, exploded, heard, scream, whole, s...\n",
       "9259    [nw, , flash, flood, warning, continued, shelb...\n",
       "9260    [, ???, , ??, , ???, , ???, mh370, , aircraft,...\n",
       "9261    [father, three, lost, control, car, overtaking...\n",
       "9262    [1, 3, , earthquake, 9km, ssw, anza, californi...\n",
       "9263    [see, 16yr, old, pkk, suicide, bomber, detonat...\n",
       "9264    [conference, attendees!, blue, line, airport, ...\n",
       "9265    [death, toll, , suicide, car, bombing, , ypg, ...\n",
       "9266    [, breaking, , la, refugio, oil, spill, may, c...\n",
       "9267          [siren, went, forney, tornado, warning, ??]\n",
       "9268    [earthquake, safety, los, angeles, , , , safet...\n",
       "9269    [official, say, quarantine, place, alabama, ho...\n",
       "9270    [, worldnews, fallen, powerlines, g, link, tra...\n",
       "9271    [flip, side, i'm, walmart, bomb, everyone, eva...\n",
       "9272    [storm, ri, worse, last, hurricane, , city, am...\n",
       "9273    [suicide, bomber, kill, 15, saudi, security, s...\n",
       "9274    [, stormchase, violent, record, breaking, ef, ...\n",
       "9275                 [green, line, derailment, chicago, ]\n",
       "9276    [two, giant, crane, holding, bridge, collapse,...\n",
       "9277    [, , control, wild, fire, california, even, no...\n",
       "9278    [police, investigating, e, bike, collided, car...\n",
       "9279    [latest, , home, razed, northern, california, ...\n",
       "9280    [meg, issue, hazardous, weather, outlook, (hwo...\n",
       "9281    [, cityofcalgary, activated, municipal, emerge...\n",
       "Name: processed_text, Length: 9282, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"processed_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(df[\"processed_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.get_item(\"flood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
