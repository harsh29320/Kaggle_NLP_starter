{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library to create a dictionaty\n",
    "import gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import training data\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/270168/Downloads/personal/nlp-starter-test/social_media_clean_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our deeds are the reason of this  earthquake m...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heard about  earthquake is different cities, s...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forest fire near la ronge sask  canada</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text choose_one  class_label\n",
       "0                 just happened a terrible car crash   Relevant            1\n",
       "1  our deeds are the reason of this  earthquake m...   Relevant            1\n",
       "2  heard about  earthquake is different cities, s...   Relevant            1\n",
       "3  there is a forest fire at spot pond, geese are...   Relevant            1\n",
       "4             forest fire near la ronge sask  canada   Relevant            1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the data set\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate text column for preprocessing\n",
    "\n",
    "df[\"processed_text\"] = df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process (G):\n",
    "    tok = G.split(\" \")\n",
    "    stop_words = stopwords.words('english')\n",
    "    no_tok = [t for t in tok if t not in stop_words]\n",
    "    filtered_tok = [tok1 for tok1 in no_tok if re.search('[a-zA-Z]', tok1)]\n",
    "    word_lemmat = WordNetLemmatizer()\n",
    "    lemmat_words = [word_lemmat.lemmatize(t) for t in filtered_tok]\n",
    "    return lemmat_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"processed_text\"] = df[\"processed_text\"].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = df[\"processed_text\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(df[\"processed_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id.get(\"flood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(t) for t in data_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    gross_count[word_id] += word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_gross_count = sorted(gross_count.items(), key=lambda w: w[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fire 443\n",
      "amp 442\n",
      "like 436\n",
      "i'm 309\n",
      "u 307\n",
      "get 303\n",
      "new 268\n",
      "one 253\n",
      "people 245\n",
      "via 235\n"
     ]
    }
   ],
   "source": [
    "#print top 10 words\n",
    "\n",
    "for word_id, word_count in sorted_gross_count[:10]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tfidf model\n",
    "\n",
    "tfidf = TfidfModel(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValuesView(<gensim.corpora.dictionary.Dictionary object at 0x0000026202CD4DA0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000, min_df=0.01, use_idf=True, tokenizer=process, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9282, 78)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accident', 'amp', 'attack', 'back', 'body', 'bomb', 'building', 'burning', 'california', \"can't\", 'car', 'collapse', 'could', 'crash', 'day', 'dead', 'death', 'disaster', 'emergency', 'even', 'fire', 'first', 'flood', 'full', 'get', 'go', 'going', 'good', 'got', 'hiroshima', 'home', 'house', \"i'm\", 'injury', 'killed', 'know', 'last', 'life', 'like', 'look', 'love', 'make', 'man', 'mass', 'need', 'never', 'new', 'news', 'nuclear', 'one', 'people', 'police', 'right', 'rt', 'say', 'see', 'service', 'still', 'storm', 'suicide', 'take', 'think', 'time', 'today', 'train', 'two', 'u', 'via', 'video', 'w', 'want', 'war', 'way', 'weapon', 'woman', 'world', 'would', 'year']\n"
     ]
    }
   ],
   "source": [
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K Means Cluster\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "n_cluster = 5\n",
    "km = KMeans(n_clusters=n_cluster)\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7971\n",
       "2     365\n",
       "4     361\n",
       "3     354\n",
       "1     231\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in Cluster 0: ['get', 'via', \"i'm\", 'new', 'u', 'news', 'people', 'emergency', 'video', 'storm']\n",
      "Words in Cluster 1: ['one', 'like', 'day', 'year', 'get', \"i'm\", 'fire', 'people', 'got', 'time']\n",
      "Words in Cluster 2: ['amp', 'rt', 'fire', 'u', 'w', 'new', 'like', 'take', 'time', \"i'm\"]\n",
      "Words in Cluster 3: ['fire', 'building', 'california', 'service', 'burning', 'u', 'two', 'say', 'news', 'time']\n",
      "Words in Cluster 4: ['like', 'look', \"i'm\", 'get', 'people', 'fire', 'would', 'burning', 'think', 'back']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#words in clusters \n",
    "\n",
    "for i in range (n_cluster):\n",
    "    print(\"Words in Cluster {}: \".format(i) , end='')\n",
    "    all_terms = []\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        all_terms.append(terms[ind])\n",
    "    print((all_terms))\n",
    "print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
